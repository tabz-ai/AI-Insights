{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUvhs6XjqK5PEHAskoRdi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabz-ai/AI-Insights/blob/main/Convolutional_Neural_Networks_(CNNs)_in_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Layer**\n",
        "This code example demonstrates the creation of a convolutional layer using TensorFlow. The Conv2D layer is used to perform convolutional operations on the input data. In this example, we create a convolutional layer with 64 filters (also known as channels), a kernel size of 3x3, ReLU activation function, and \"same\" padding to ensure the output has the same spatial dimensions as the input. The input_shape parameter specifies the shape of the input data, which is (32, 32, 3) in this case (32x32 pixels with 3 color channels)."
      ],
      "metadata": {
        "id": "9-feiitTWG0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OF4QjiomSvff"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a convolutional layer\n",
        "conv_layer = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(32, 32, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pooling Layer**\n",
        "\n",
        "This code example demonstrates the creation of a pooling layer using TensorFlow. The MaxPooling2D layer is used to downsample the input data by selecting the maximum value within a specific pool size. In this example, we create a pooling layer with a pool size of 2x2. The pooling layer reduces the spatial dimensions of the input data, which helps in extracting important features while reducing computational complexity."
      ],
      "metadata": {
        "id": "kEZEu1fvWVZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a pooling layer\n",
        "pool_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n"
      ],
      "metadata": {
        "id": "Os1pFKSxWZ6l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fully Connected Layer **\n",
        "\n",
        "This code example showcases the creation of a fully connected layer using TensorFlow. The Dense layer represents a fully connected layer, where each neuron is connected to every neuron in the previous layer. In this example, we create a fully connected layer with 128 neurons and ReLU activation function. The units parameter specifies the number of neurons in the layer."
      ],
      "metadata": {
        "id": "D8xPqiSkWeiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a fully connected layer\n",
        "fc_layer = tf.keras.layers.Dense(units=128, activation='relu')\n"
      ],
      "metadata": {
        "id": "u3M9snDfWgwp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training a CNN**\n",
        "\n",
        "This code example demonstrates the process of training a CNN using TensorFlow. It starts by loading and preprocessing a dataset (e.g., CIFAR-10). The input data is normalized by dividing each pixel value by 255 to bring it into the range of 0 to 1. Then, a CNN model is defined using the Sequential API from TensorFlow. The model consists of a convolutional layer, pooling layer, flattening layer, fully connected layer, and a final output layer. The model is compiled with the Adam optimizer, sparse categorical cross-entropy loss function, and accuracy metric.\n",
        "\n",
        "The model is trained by calling the fit method on the model object, providing the training data, labels, and the number of epochs. The validation data (x_test, y_test) is used to evaluate the model's performance after each epoch. The training process adjusts the weights and biases of the network to minimize the loss and improve accuracy."
      ],
      "metadata": {
        "id": "W_YRa3enWni_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Define the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhhmXNNDWm2q",
        "outputId": "df914125-7ff0-4cd4-99d8-dbb1ee7d38b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 62s 39ms/step - loss: 1.5099 - accuracy: 0.4634 - val_loss: 1.2782 - val_accuracy: 0.5492\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 54s 35ms/step - loss: 1.2010 - accuracy: 0.5791 - val_loss: 1.1568 - val_accuracy: 0.5924\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 1.0896 - accuracy: 0.6232 - val_loss: 1.1501 - val_accuracy: 0.5999\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 1.0142 - accuracy: 0.6479 - val_loss: 1.1127 - val_accuracy: 0.6108\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.9546 - accuracy: 0.6670 - val_loss: 1.0631 - val_accuracy: 0.6373\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 60s 38ms/step - loss: 0.9050 - accuracy: 0.6858 - val_loss: 1.0456 - val_accuracy: 0.6383\n",
            "Epoch 7/10\n",
            "1176/1563 [=====================>........] - ETA: 11s - loss: 0.8541 - accuracy: 0.7055"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a Pretrained Model for Transfer Learning**\n",
        "\n",
        "\n",
        "This code example showcases the utilization of a pretrained model for transfer learning using TensorFlow. First, a pretrained model (e.g., MobileNetV2) is loaded from TensorFlow's model zoo. The weights parameter is set to 'imagenet' to load the weights trained on the ImageNet dataset. The include_top parameter is set to False to exclude the original classifier on top of the model.\n",
        "\n",
        "Next, the pretrained layers are frozen to prevent their weights from being updated during training. This is done by iterating through the layers and setting their trainable attribute to False.\n",
        "\n",
        "Custom layers are added on top of the pretrained model to create a new classifier suitable for the target task. In this example, a global average pooling layer, a dense layer with ReLU activation, and a final dense layer with softmax activation are added. The global average pooling layer reduces the spatial dimensions of the output from the pretrained model, allowing the addition of the dense layers.\n",
        "\n",
        "The model is compiled with the Adam optimizer, sparse categorical cross-entropy loss function, and accuracy metric. It is then trained on a new dataset (new_x_train, new_y_train) by calling the fit method. The performance of the model is evaluated using the testing dataset (x_test, y_test) to assess its final accuracy.\n",
        "\n",
        "These code explainers provide insights into the implementation details of the code examples and help readers understand the functionality and purpose of each code snippet within the context of CNNs and deep learning."
      ],
      "metadata": {
        "id": "tIwEE9dLWyIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load a pretrained model (e.g., MobileNetV2)\n",
        "pretrained_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pretrained layers\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top of the pretrained model\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model on a new dataset\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(new_x_train, new_y_train, epochs=10, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "FusMrd24Wy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**\n",
        "\n",
        "Data augmentation is a technique used to artificially increase the diversity of the training dataset by applying various transformations to the existing images. It helps to reduce overfitting and improve the generalization capability of the CNN. Here's an example of data augmentation using the ImageDataGenerator class in Keras:"
      ],
      "metadata": {
        "id": "BczuqQhJW3r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an instance of ImageDataGenerator with augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate augmented images\n",
        "augmented_images = datagen.flow(x_train, y_train, batch_size=32)\n"
      ],
      "metadata": {
        "id": "J8mqKLf5XxEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, various transformations are applied to the input images, including rotation, horizontal and vertical shifts, shear, zoom, and horizontal flip. The flow method generates augmented images in batches for training.\n",
        "\n",
        "Fine-tuning a Pretrained Model\n",
        "Fine-tuning involves adapting a pretrained model to a new task by unfreezing some of its layers and training the entire model with a smaller learning rate. Here's an example of fine-tuning a pretrained model in TensorFlow:\n",
        "\n"
      ],
      "metadata": {
        "id": "hgr_ln2XXxuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load a pretrained model (e.g., MobileNetV2) without top layers\n",
        "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze most of the layers in the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom top layers for the new task\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model with a smaller learning rate\n",
        "model.fit(train_data, epochs=10, validation_data=val_data)\n"
      ],
      "metadata": {
        "id": "_3H3gfFxX55z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, a pretrained MobileNetV2 model is loaded without the top layers. The base model layers are frozen to retain their learned representations. Custom top layers are added to adapt the model to a new task, followed by creating the final model. The model is compiled with an appropriate optimizer and loss function, and then fine-tuned on the new dataset with a smaller learning rate.\n",
        "\n",
        "These additional code samples provide insights into important aspects of CNNs, such as data augmentation to improve training data diversity and fine-tuning a pretrained model for transfer learning."
      ],
      "metadata": {
        "id": "iwMePRUQX6jw"
      }
    }
  ]
}