{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs1OB45lKZENyPnqdToNtZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tabz-ai/AI-Insights/blob/main/Introduction_to_Machine_Learning_Classification_Algorithims.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Algorithms**\n",
        "\n",
        "In machine learning, classification algorithms are used to predict the class or category of a data point based on its features. \n",
        "They are widely used in image recognition, natural language processing, and other fields where classification is important. \n",
        "Common classification algorithms include:\n",
        "\n",
        "**1. Decision Trees:**\n",
        "Decision trees are a simple and intuitive classification algorithm that works by splitting the data into subsets based on the values of\n",
        "the input features. Each split is chosen based on the feature that best separates the data into the different classes. The resulting tree \n",
        "is a set of decision rules that can be used to classify new data points. Decision trees are easy to interpret and can handle both categorical \n",
        "and continuous input features.\n",
        "\n",
        "To implement a decision tree classifier:\n",
        "\n"
      ],
      "metadata": {
        "id": "8HT2yideLJBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# create a decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# train the classifier on the training data\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = dtc.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z9rUj4yLb9p",
        "outputId": "e6d4ada4-09ec-46d7-f248-34172b9fffba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the decision tree classifier is the predicted class label for each instance in the test set. The accuracy score indicates how well the classifier has performed in terms of correctly classifying the test instances.\n",
        "\n",
        "In the code example above, we split the iris dataset into a training set and a test set using the train_test_split() function from scikit-learn. We trained the decision tree classifier on the training set using the fit() method, and then made predictions on the test set using the predict() method. The accuracy of the classifier was then calculated using the accuracy_score() function, which compares the predicted class labels with the actual class labels in the test set.\n",
        "\n",
        "In machine learning, decision trees are commonly used as a classification algorithm for both binary and multi-class classification problems. They are particularly useful when the data has a mixture of categorical and numerical features, and when the goal is to create a model that is easy to interpret and explain to others. Decision trees can also be used for regression problems by predicting a continuous output instead of a categorical one.\n",
        "\n",
        "Decision trees can be used as standalone models or as part of a larger ensemble of models, such as random forests or gradient boosted trees. They can be used in a variety of applications, such as credit risk analysis, customer segmentation, fraud detection, and medical diagnosis, among others.\n",
        "\n",
        "**2. Random Forests:**\n",
        "Random forests are an ensemble of decision trees that work by creating multiple decision trees and combining their results. Each tree is \n",
        "trained on a random subset of the data and a random subset of the features. The final result is obtained by averaging the results of all the trees. \n",
        "Random forests are less prone to overfitting than decision trees and can handle high-dimensional data with many features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0_-HPonMJDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# create a random forest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# train the classifier on the training data\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa43TfMKNVIa",
        "outputId": "6914932b-54fa-403a-a329-870d1c768ce4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the random forest classifier is the predicted class label for each instance in the test set. The accuracy score indicates how well the classifier has performed in terms of correctly classifying the test instances.\n",
        "\n",
        "In the code example above, we split the iris dataset into a training set and a test set using the train_test_split() function from scikit-learn. We created a random forest classifier with 100 decision trees using the RandomForestClassifier() constructor, and trained the classifier on the training set using the fit() method. We then made predictions on the test set using the predict() method, and calculated the accuracy of the classifier using the accuracy_score() function, which compares the predicted class labels with the actual class labels in the test set.\n",
        "\n",
        "The accuracy score can range from 0 to 1, with a higher score indicating better performance. In this example, the accuracy of the random forest classifier on the test set is printed to the console. This gives an idea of how well the model is likely to perform when used to classify new, unseen data.\n",
        "\n",
        "In machine learning, random forests are commonly used as a classification algorithm for both binary and multi-class classification problems, as well as for feature selection. They can be used in a variety of applications, such as computer vision, natural language processing, and bioinformatics, among others.\n",
        "\n",
        "**3. Support Vector Machines (SVMs):**\n",
        "Support vector machines are a powerful and flexible classification algorithm that works by finding the hyperplane that best separates the \n",
        "data into the different classes. The hyperplane is chosen to maximize the margin between the classes, which makes the classifier more robust to\n",
        "noise and outliers. SVMs can handle both linear and non-linear decision boundaries and can be used with different types of kernels to map the input \n",
        "features into a higher-dimensional space.\n",
        "\n"
      ],
      "metadata": {
        "id": "v10Vz-_2Nesv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# create a SVM classifier\n",
        "svc = SVC(kernel='linear')\n",
        "\n",
        "# train the classifier on the training data\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-du9LNFN6yd",
        "outputId": "f62fb7a1-768a-4b3d-a379-8155a302efc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the iris dataset and split it into training and testing sets. We then create an SVM classifier with a linear kernel and train it on the training data. We make predictions on the test data and calculate the accuracy of the classifier. Finally, we print the accuracy score.\n",
        "\n",
        "The SVC constructor in scikit-learn allows for several different kernels to be used, including linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel will depend on the characteristics of the data being classified. In this example, we use a linear kernel, which is appropriate for linearly separable data.\n",
        "\n",
        "The SVM classifier works by finding the hyperplane that best separates the data into the different classes. The hyperplane is chosen to maximize the margin between the classes, which makes the classifier more robust to noise and outliers. If the data is not linearly separable, a non-linear kernel can be used to map the input features into a higher-dimensional space where linear separation is possible.\n",
        "\n",
        "In terms of performance, SVMs are often used in machine learning because they can produce highly accurate models that are robust to noise and can handle large datasets with many features. They can also be used for regression, outlier detection, and feature selection. Overall, SVMs are a powerful and flexible algorithm that can be used in a wide variety of applications.\n",
        "\n",
        "**4. Naive Bayes:**\n",
        "Naive Bayes is a simple and efficient classification algorithm that works by applying Bayes' theorem to calculate the probability of \n",
        "each class given the input features. The algorithm assumes that the input features are independent of each other, which is why it is called \"naive\". \n",
        "Despite its simplicity, Naive Bayes can be surprisingly accurate, especially for high-dimensional data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W-vzSk8rN8M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# create a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# train the classifier on the training data\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpo2KP06OKz3",
        "outputId": "a82a046b-5316-49f2-d164-3133140a2dea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the iris dataset and split it into training and testing sets. We then create a Gaussian Naive Bayes classifier and train it on the training data. We make predictions on the test data and calculate the accuracy of the classifier. Finally, we print the accuracy score.\n",
        "\n",
        "The GaussianNB constructor in scikit-learn assumes that the input features are normally distributed and uses Bayes' theorem to calculate the probability of each class given the input features. Despite the assumption of independence among the input features, Naive Bayes can perform surprisingly well in practice, especially for high-dimensional data.\n",
        "\n",
        "In terms of performance, Naive Bayes classifiers are often used in machine learning because they are simple and computationally efficient, making them particularly well-suited for large datasets. They can also be used for text classification, spam filtering, and sentiment analysis, among other applications. Overall, Naive Bayes is a useful and versatile algorithm for many different classification problems.\n",
        "\n",
        "**5. K-Nearest Neighbors (KNN):**\n",
        "K-Nearest Neighbors is a lazy classification algorithm that works by finding the k nearest neighbors of a data point in the training set and \n",
        "assigning it the class that is most common among its neighbors. The algorithm can work with any type of distance metric and can handle both numerical \n",
        "and categorical input features. KNN is simple to implement and can work well for low-dimensional data with simple decision boundaries."
      ],
      "metadata": {
        "id": "l67ZV2MROPSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# create a KNN classifier with k=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# train the classifier on the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AcjNbFbOY8D",
        "outputId": "b48acf2f-6dd3-42fc-8802-8ad4240b9966"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the iris dataset and split it into training and testing sets. We then create a KNN classifier with k=3 and train it on the training data. We make predictions on the test data and calculate the accuracy of the classifier. Finally, we print the accuracy score.\n",
        "\n",
        "The KNeighborsClassifier constructor in scikit-learn takes the number of nearest neighbors k as a parameter and uses a distance metric (by default, Euclidean distance) to find the k nearest neighbors of a data point in the training set. The class of the data point is then assigned to the most common class among its neighbors.\n",
        "\n",
        "KNN can be used for both classification and regression problems and can work with any type of distance metric. However, it can be computationally expensive for large datasets and may not work well for high-dimensional data with complex decision boundaries. Overall, KNN is a useful and versatile algorithm for many different classification problems, especially for low-dimensional data with simple decision boundaries.\n",
        "\n",
        "**6. Neural Networks:**\n",
        "Neural networks are a powerful and flexible classification algorithm that works by simulating the behavior of neurons in the human brain. \n",
        "The network is made up of multiple layers of interconnected nodes, where each node performs a simple computation on its input and passes the \n",
        "result to the next layer. The network is trained by adjusting the weights of the connections to minimize the error between the predicted and \n",
        "actual outputs. Neural networks can handle complex non-linear decision boundaries and can work well for high-dimensional data with many features. \n",
        "However, they can be difficult to train and interpret.\n",
        "\n"
      ],
      "metadata": {
        "id": "4A-ET0DWOlzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# convert the labels to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# create a neural network with two hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=4, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# compile the model with categorical cross-entropy loss and adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model on the training data\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
        "\n",
        "# make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# convert the predictions back to class labels\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "# calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test.argmax(axis=1), y_pred)\n",
        "\n",
        "# print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQm3HpA8Oty3",
        "outputId": "ee0b6f7a-b9c8-4e29-fdbf-1dd0f0971c99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first load the iris dataset and split it into training and testing sets. We then convert the labels to one-hot encoded vectors, which is necessary for training the neural network. We create a neural network with two hidden layers using the Sequential model in Keras. The first hidden layer has 10 nodes and uses the ReLU activation function. The second hidden layer has 8 nodes and also uses the ReLU activation function. The output layer has 3 nodes (one for each class) and uses the softmax activation function to produce a probability distribution over the classes.\n",
        "\n",
        "We compile the model with categorical cross-entropy loss (since we are doing multi-class classification) and the Adam optimizer. We train the model on the training data using the fit method, specifying the number of epochs (i.e., the number of times the entire training set is passed through the network) and the batch size (i.e., the number of training examples to use for each update of the weights).\n",
        "\n",
        "We make predictions on the test data using the predict method, which returns a probability distribution over the classes for each test example. We convert the predictions back to class labels by taking the index of the maximum probability. Finally, we calculate the accuracy of the classifier using scikit-learn's accuracy_score function.\n",
        "\n",
        "Neural networks can be used for both classification and regression problems and can handle complex non-linear decision boundaries. They can also be used for unsupervised learning tasks such as clustering and dimensionality reduction. However, they can be difficult to train and interpret, and require large amounts of data and computational resources. Overall, neural networks are a powerful and versatile algorithm for many different machine learning problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "PB0WL2tOO2p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random data\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(500, 2)\n",
        "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Train different models on the data\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "models = [\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    SVC(),\n",
        "    GaussianNB(),\n",
        "    KNeighborsClassifier(),\n",
        "    MLPClassifier()\n",
        "]\n",
        "\n",
        "model_names = [\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"SVM\",\n",
        "    \"Naive Bayes\",\n",
        "    \"KNN\",\n",
        "    \"Neural Network\"\n",
        "]\n",
        "\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(np.arange(len(models)), train_scores, width=0.4, label=\"Training\")\n",
        "ax.bar(np.arange(len(models)) + 0.4, test_scores, width=0.4, label=\"Test\")\n",
        "ax.set_xticks(np.arange(len(models)) + 0.2)\n",
        "ax.set_xticklabels(model_names, rotation=45)\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "PqXV7soBPQGe",
        "outputId": "45b3558b-8653-45fe-d0bd-ff42bf9fcd53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cr7Tx113PXRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates some random data that is not linearly separable and splits it into a training set and a test set. It then trains six different models on the data: a decision tree, a random forest, an SVM, a naive Bayes classifier, a KNN classifier, and a neural network. For each model, it computes the accuracy on the training set and the test set and stores these scores in two lists.\n",
        "\n",
        "Finally, it plots a bar chart showing the training and test accuracy for each model:\n",
        "\n",
        "From this plot, we can see that the neural network has the highest training and test accuracy, followed closely by the SVM and the random forest. The decision tree and the naive Bayes classifier have lower accuracy, while the KNN classifier has the lowest accuracy. However, these results may vary depending on the specific dataset and problem being studied, so it's always important to evaluate multiple models and choose the one that performs best on the specific task at hand.\n"
      ],
      "metadata": {
        "id": "awmIJ904PXkq"
      }
    }
  ]
}
